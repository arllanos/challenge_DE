{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration & Analysis using Spark\n",
    "\n",
    "**IMPORTANTE**: Este es un Jupyter notebook complementario usado para explorar datos y validar resultados. Para la solucion oficial al Challenge referirse al notebook [challenge.ipynb](./challenge.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisitos\n",
    "\n",
    "\n",
    "1. Si Spark no está instalado, hacerlo siguiendo las instrucciones en el [sitio oficial de Spark](https://spark.apache.org/downloads.html)\n",
    "\n",
    "2. Asegurarse de actualizar las variables de entorno `SPARK_HOME` y `PATH`\n",
    "    ```bash\n",
    "    export SPARK_HOME=/path/to/spark\n",
    "    export PATH=$PATH:$SPARK_HOME/bin\n",
    "    ```\n",
    "\n",
    "3. Create a virtual environment (`.venv`) in the root directory and install all the project dependencies.\n",
    "    ```sh\n",
    "    python3 -m venv .venv\n",
    "    source .venv/bin/activate\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "    \n",
    "4. Download the data\n",
    "\n",
    "    Descarga manualmente https://drive.google.com/file/d/1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis/view?usp=sharing, y extrae del archivo `.zip` el json file.\n",
    "    El archivo extraído debe ser copiado a la carpeta `data/`.\n",
    "\n",
    "\n",
    "**IMPORTANTE**: No es necesario hacer **3** y **4** si ya lo hizo como parte del setup indicado en el notebook [challenge.ipynb](./challenge.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of Notebook and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable the autoreload extension and configure it for automatic module's reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Twitter Data Analysis\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tweets\")\n",
    "spark.sql(\"SELECT * FROM tweets LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentioned = spark.sql(\"\"\"\n",
    "            SELECT id, retweetedTweet, content, mentioned.username AS username\n",
    "            FROM tweets\n",
    "            LATERAL VIEW explode(mentionedUsers) t AS mentioned\n",
    "            WHERE mentioned.username = 'meenaharris'\n",
    "            ORDER BY id\n",
    "          \"\"\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentioned.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark and SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Top 10 Dates with Most Tweets and the User with Most Tweets on Each Date\n",
    "\n",
    "Approach: Using pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from typing import List, Tuple\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Twitter Data Analysis\").getOrCreate()\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    tweets_by_date = df.withColumn(\"date\", to_date(col(\"date\"))).groupBy(\"date\").count()\n",
    "\n",
    "    top_dates = tweets_by_date.orderBy(col(\"count\").desc()).limit(10).collect()\n",
    "\n",
    "    results = []\n",
    "    for row in top_dates:\n",
    "        date = row[\"date\"]\n",
    "\n",
    "        top_user = df.filter(col(\"date\") == date).groupBy(\"user.username\").count().orderBy(col(\"count\").desc()).first()\n",
    "        results.append((date, top_user[\"username\"]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Get top 10 emojis more used with its corresponding count\n",
    "\n",
    "Approach: Using pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from typing import List, Tuple\n",
    "from pyspark.sql.functions import udf, explode, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Twitter Data Analysis\").getOrCreate()\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emojis = emoji.emoji_list(text)\n",
    "    return [e['emoji'] for e in emojis]\n",
    "\n",
    "extract_emojis_udf = udf(extract_emojis, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    emojis_df = df.withColumn(\"emojis\", extract_emojis_udf(col(\"content\")))\n",
    "    emojis_exploded = emojis_df.select(explode(col(\"emojis\")).alias(\"emoji\"))\n",
    "    emoji_counts = emojis_exploded.groupBy(\"emoji\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "    return [(row['emoji'], row['count']) for row in emoji_counts.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Top 10 most influential historical users (username) based on the count of mentions (@) recorded by each of them\n",
    "\n",
    "Approach: Simple query using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT mentioned.username AS username, COUNT(*) AS mentions_count\n",
    "            FROM tweets\n",
    "            LATERAL VIEW explode(mentionedUsers) t AS mentioned\n",
    "            GROUP BY mentioned.username\n",
    "            ORDER BY mentions_count DESC\n",
    "          \"\"\"\n",
    "          ).show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
